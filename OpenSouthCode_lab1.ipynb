{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "AtqNgVcKqZxP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "ae9d6495-af97-405d-b4d6-63b322cb82d5"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U torch datasets transformers tensorflow langchain playwright html2text sentence_transformers faiss-cpu\n",
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 trl==0.4.7"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community"
      ],
      "metadata": {
        "id": "bJcDpwF9Ib2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "PUBINayXIfn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More libraries"
      ],
      "metadata": {
        "id": "_LBvozC5qsd6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "505ca9a3-8c27-442e-bca6-154a65186d01"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel\n",
        "\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_transformers import Html2TextTransformer\n",
        "from langchain.document_loaders import AsyncChromiumLoader\n",
        "\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import LLMChain\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the model"
      ],
      "metadata": {
        "id": "kRwgjH53q6uE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "80f94e97-7e58-4253-9376-73af6f36e139"
      },
      "outputs": [],
      "source": [
        "model_name='mistralai/Mistral-7B-Instruct-v0.1'\n",
        "#model_name='google/gemma-7b'\n",
        "# TODO USE YOUR OWN PERSONAL TOKEN\n",
        "login(token=userdata.get(\"HuggingFaceHub\"))\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "#################################################################\n",
        "# bitsandbytes parameters\n",
        "#################################################################\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False\n",
        "\n",
        "#################################################################\n",
        "# Set up quantization config\n",
        "#################################################################\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "#################################################################\n",
        "# Load pre-trained config\n",
        "#################################################################\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainable parameters"
      ],
      "metadata": {
        "id": "X9RJ9RHosHh9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "91d2a86e-69e8-496f-b388-853168537c20"
      },
      "outputs": [],
      "source": [
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
        "\n",
        "print(print_number_of_trainable_model_parameters(model))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text generation pipeline"
      ],
      "metadata": {
        "id": "j8oAYC0VyPRP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "8c613429-9e6c-4a1e-bc9c-579eb152434b"
      },
      "outputs": [],
      "source": [
        "text_generation_pipeline = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True,\n",
        "    max_new_tokens=1000,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "c859dd05-9114-42f1-81f2-52a28b7efdd7"
      },
      "outputs": [],
      "source": [
        "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing to parse new data sources"
      ],
      "metadata": {
        "id": "froJ_OasLGD1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "id": "35e625a4-8d25-453e-bef0-435a6e1aa135"
      },
      "outputs": [],
      "source": [
        "!playwright install\n",
        "!playwright install-deps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "79a2e41f-aee3-47ff-92a1-74970f3b313a"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Articles to index\n",
        "injection_articles = [\"https://wuzzi.net/ai-tests/einstein-webpilot.html\"]\n",
        "\n",
        "# Scrapes the blogs above\n",
        "loader = AsyncChromiumLoader(injection_articles)\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "id": "ff328fea-b7c7-4ca3-915c-39c0ebaa2f7a"
      },
      "outputs": [],
      "source": [
        "# Converts HTML to plain text\n",
        "html2text = Html2TextTransformer()\n",
        "docs_transformed = html2text.transform_documents(docs)\n",
        "\n",
        "# Chunk text\n",
        "text_splitter = CharacterTextSplitter(chunk_size=100,\n",
        "                                      chunk_overlap=0)\n",
        "chunked_documents = text_splitter.split_documents(docs_transformed)\n",
        "\n",
        "# Load chunked documents into the FAISS index\n",
        "db = FAISS.from_documents(chunked_documents,\n",
        "                          HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'))\n",
        "\n",
        "retriever = db.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Prompt template and langchain"
      ],
      "metadata": {
        "id": "-ZMYsyvUMuaV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "3bd688c2-25ac-4d65-88c6-635f9c95ada4"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"\n",
        "### [INST] Instruction: Answer the question based on your wuzzi einstein webpilot knowledge. Here is context to help:\n",
        "\n",
        "{context}\n",
        "\n",
        "### QUESTION:\n",
        "{question} [/INST]\n",
        " \"\"\"\n",
        "\n",
        "# Create prompt from prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=prompt_template,\n",
        ")\n",
        "\n",
        "# Create llm chain\n",
        "llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"context\": \"\", \"question\": \"How could you ensure Einstein is happy\"})"
      ],
      "metadata": {
        "id": "aJPfZ_I5R8xP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Chain"
      ],
      "metadata": {
        "id": "MB2Mt-mDUpEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain = (\n",
        " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | llm_chain\n",
        ")\n",
        "\n",
        "result = rag_chain.invoke(\"How could you ensure Einstein is happy\")"
      ],
      "metadata": {
        "id": "skd4__UgUqd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[\"context\"]"
      ],
      "metadata": {
        "id": "LGwWw4CRUPsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['text'])"
      ],
      "metadata": {
        "id": "V7Uzi1siUPvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XMcGEk5bUP0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resources\n",
        "https://arxiv.org/pdf/2312.14197.pdf\n"
      ],
      "metadata": {
        "id": "Mo9K54aOPYd6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GLyZJ8upJcFd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}