{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing libraries"
      ],
      "metadata": {
        "id": "pwTva6zRb7hO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade optimum[onnxruntime]"
      ],
      "metadata": {
        "id": "2gbh0iTZbLet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from optimum.onnxruntime import ORTModelForSequenceClassification"
      ],
      "metadata": {
        "id": "vDrIwrxWMgvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet langchain transformers"
      ],
      "metadata": {
        "id": "dpehIeHqMhdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Needed classes for the Prompt Injection detection"
      ],
      "metadata": {
        "id": "Bl5BT_XXcEtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from typing import TYPE_CHECKING, Any\n",
        "\n",
        "from langchain.pydantic_v1 import Field, root_validator\n",
        "from langchain.tools.base import BaseTool\n",
        "\n",
        "if TYPE_CHECKING:\n",
        "    from transformers import Pipeline"
      ],
      "metadata": {
        "id": "eVnP9Jc2t0oO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Tool for the identification of prompt injection attacks.\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "class PromptInjectionException(ValueError):\n",
        "    \"\"\"Exception raised when prompt injection attack is detected.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, message: str = \"Prompt injection attack detected\", score: float = 1.0\n",
        "    ):\n",
        "        self.message = message\n",
        "        self.score = score\n",
        "\n",
        "        super().__init__(self.message)\n",
        "\n",
        "\n",
        "def _model_default_factory(\n",
        "    model_name: str = \"protectai/deberta-v3-base-prompt-injection-v2\",\n",
        ") -> Pipeline:\n",
        "    try:\n",
        "        from transformers import (\n",
        "            AutoModelForSequenceClassification,\n",
        "            AutoTokenizer,\n",
        "            pipeline,\n",
        "        )\n",
        "    except ImportError as e:\n",
        "        raise ImportError(\n",
        "            \"Cannot import transformers, please install with \"\n",
        "            \"`pip install transformers`.\"\n",
        "        ) from e\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "    return pipeline(\n",
        "        \"text-classification\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=512,  # default length of BERT models\n",
        "        truncation=True,  # otherwise it will fail on long prompts\n",
        "    )\n",
        "\n",
        "\n",
        "class HuggingFaceInjectionIdentifier(BaseTool):\n",
        "    \"\"\"Tool that uses HuggingFace Prompt Injection model to\n",
        "    detect prompt injection attacks.\"\"\"\n",
        "\n",
        "    name: str = \"hugging_face_injection_identifier\"\n",
        "    description: str = (\n",
        "        \"A wrapper around HuggingFace Prompt Injection security model. \"\n",
        "        \"Useful for when you need to ensure that prompt is free of injection attacks. \"\n",
        "        \"Input should be any message from the user.\"\n",
        "    )\n",
        "    model: Any = Field(default_factory=_model_default_factory)\n",
        "    \"\"\"Model to use for prompt injection detection.\n",
        "\n",
        "    Can be specified as transformers Pipeline or string. String should correspond to the\n",
        "        model name of a text-classification transformers model. Defaults to\n",
        "        ``protectai/deberta-v3-base-prompt-injection-v2`` model.\n",
        "    \"\"\"\n",
        "    threshold: float = Field(\n",
        "        description=\"Threshold for prompt injection detection.\", default=0.5\n",
        "    )\n",
        "    \"\"\"Threshold for prompt injection detection.\n",
        "\n",
        "    Defaults to 0.5.\"\"\"\n",
        "    injection_label: str = Field(\n",
        "        description=\"Label of the injection for prompt injection detection.\",\n",
        "        default=\"INJECTION\",\n",
        "    )\n",
        "    \"\"\"Label for prompt injection detection model.\n",
        "\n",
        "    Defaults to ``INJECTION``. Value depends on the model used.\"\"\"\n",
        "\n",
        "    @root_validator(pre=True)\n",
        "    def validate_environment(cls, values: dict) -> dict:\n",
        "        if isinstance(values.get(\"model\"), str):\n",
        "            values[\"model\"] = _model_default_factory(model_name=values[\"model\"])\n",
        "        return values\n",
        "\n",
        "    def _run(self, query: str) -> str:\n",
        "        \"\"\"Use the tool.\"\"\"\n",
        "        result = self.model(query)  # type: ignore\n",
        "        score = (\n",
        "            result[0][\"score\"]\n",
        "            if result[0][\"label\"] == self.injection_label\n",
        "            else 1 - result[0][\"score\"]\n",
        "        )\n",
        "        if score > self.threshold:\n",
        "            raise PromptInjectionException(\"Prompt injection attack detected\", score)\n",
        "\n",
        "        return query\n",
        "\n",
        "\n",
        "HuggingFaceInjectionIdentifier.update_forward_refs()\n"
      ],
      "metadata": {
        "id": "aCWPiYgVt_oM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yH0pWqry1Hu"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsFEp7xnzN7-"
      },
      "source": [
        "# Loading prompt injection identifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91b1dszqy1K-"
      },
      "outputs": [],
      "source": [
        "# Using https://huggingface.co/protectai/deberta-v3-base-prompt-injection-v2\n",
        "model_path = \"protectai/deberta-v3-base-prompt-injection-v2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_path, model_input_names=[\"input_ids\", \"attention_mask\"], subfolder=\"onnx\"\n",
        ")\n",
        "model = ORTModelForSequenceClassification.from_pretrained(\n",
        "    model_path, subfolder=\"onnx\"\n",
        ")\n",
        "\n",
        "classifier = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        ")\n",
        "\n",
        "HuggingFaceInjectionIdentifier.update_forward_refs()\n",
        "injection_identifier = HuggingFaceInjectionIdentifier(\n",
        "    model=classifier,\n",
        ")\n",
        "\n",
        "injection_identifier.name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GTcPQ89zUoc"
      },
      "source": [
        "# Preparing to parse data sources"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U torch datasets tensorflow playwright html2text sentence_transformers faiss-cpu\n",
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 trl==0.4.7"
      ],
      "metadata": {
        "id": "9zOQ9T8Oc6Rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community"
      ],
      "metadata": {
        "id": "Ht86q3dIvXPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlO5dkVky1Oh"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_transformers import Html2TextTransformer\n",
        "from langchain.document_loaders import AsyncChromiumLoader\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import LLMChain\n",
        "import nest_asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVAvilYQzYBA"
      },
      "outputs": [],
      "source": [
        "!playwright install\n",
        "!playwright install-deps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i27UBiHFzYD7"
      },
      "outputs": [],
      "source": [
        "nest_asyncio.apply()\n",
        "\n",
        "# Articles to index\n",
        "injection_articles = [\"https://wuzzi.net/ai-tests/einstein-webpilot.html\"]\n",
        "\n",
        "# Scrapes the blogs above\n",
        "loader = AsyncChromiumLoader(injection_articles)\n",
        "docs = loader.load()\n",
        "\n",
        "# Converts HTML to plain text\n",
        "html2text = Html2TextTransformer()\n",
        "docs_transformed = html2text.transform_documents(docs)\n",
        "\n",
        "# Chunk text\n",
        "text_splitter = CharacterTextSplitter(chunk_size=100,\n",
        "                                      chunk_overlap=0)\n",
        "chunked_documents = text_splitter.split_documents(docs_transformed)\n",
        "\n",
        "# Load chunked documents into the FAISS index\n",
        "db = FAISS.from_documents(chunked_documents,\n",
        "                          HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'))\n",
        "\n",
        "retriever = db.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVcDa9hvzjuA"
      },
      "source": [
        "# Testing the identifier without context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PDQ9jZlzfyI"
      },
      "outputs": [],
      "source": [
        "injection_identifier.run(\"How could you ensure Einstein is happy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHvNsNjvzu4m"
      },
      "source": [
        "# Try again passing the context of the retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dwapyKdzf72"
      },
      "outputs": [],
      "source": [
        "from string import Template\n",
        "prompt_template = Template(\n",
        "    \"\"\"\n",
        "### [INST] Instruction: Answer the question based on your wuzzi einstein webpilot knowledge. Here is context to help:\n",
        "\n",
        "{context}\n",
        "\n",
        "### QUESTION:\n",
        "{question} [/INST]\n",
        " \"\"\"\n",
        ")\n",
        "\n",
        "final_prompt = prompt_template.substitute(\n",
        "        context=retriever,\n",
        "        question=\"How could you ensure Einstein is happy\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "injection_identifier.run(final_prompt)"
      ],
      "metadata": {
        "id": "xWFR7sUQNdM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_1SO-xGz5mX"
      },
      "source": [
        "## As Exercise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7FoCzUZz7RE"
      },
      "outputs": [],
      "source": [
        "# TODO create the same llm_chain as in Lab1\n",
        "rag_chain = (\n",
        " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | llm_chain\n",
        ")\n",
        "new_chain = injection_identifier | rag_chain\n",
        "result = new_chain.invoke(\"How could you ensure Einstein is happy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EFeTkbe0Mo1"
      },
      "source": [
        "# Resources\n",
        "\n",
        "https://arxiv.org/pdf/2312.14197.pdf\n",
        "\n",
        "https://huggingface.co/protectai/deberta-v3-base-prompt-injection-v2\n",
        "\n",
        "https://github.com/langchain-ai/langchain/discussions/19995\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}